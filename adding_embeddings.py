# -*- coding: utf-8 -*-
"""adding_embeddings.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ap0ichyRX0ILP_1V1BOU-mZ6YjvhhL9u
"""

import pandas as pd
from tqdm import tqdm
import string
tqdm.pandas()
import spacy
import numpy as np
from sentence_transformers import SentenceTransformer, util
import matplotlib.pyplot as plt
import seaborn as sns
from transformers import BertTokenizer
import nltk
from nltk.corpus import stopwords
import string

import kagglehub

# Download latest version
path = kagglehub.dataset_download("jainamgada45/indian-government-schemes")

print("Path to dataset files:", path)

data=pd.read_csv(path + "/updated_data.csv")
data.head()

plt.figure(figsize=(8, 6))
sns.countplot(x='level', data=data)
plt.title('Distribution of Schemes by Level')
plt.xlabel('Level')
plt.ylabel('Number of Schemes')
plt.show()

"""FOR THIS MVP,**LETS WORK ONLY ON CENTRAL LEVEL SCHEMES**"""

df=data[data['level']=="Central"].copy()
df=df.drop(columns=['slug','eligibility','application','level','Unnamed: 9'])

"""I wanna make a **columns named tokens, now this tokens should include everything in the other columns except schemes**.Coz all the words used in them can be used a keyword, whick we will compare to the input"""

df = df.reset_index(drop=True)

df['tokens'] = df['details'] + ' ' + df['benefits'] + ' ' + df['schemeCategory'] + ' ' + df['tags']

df.head()

"""We will use bert tokenizer here unlike spacy that we did earlier"""

# nltk.download('stopwords')
# stopwords = set(stopwords.words('english'))


# tokenizer = BertTokenizer.from_pretrained("bert-base-cased")
# text = 'ChatGPT is a language model developed by OpenAI, based on the GPT (Generative Pre-trained Transformer) architecture. '

# # Tokenize and encode the text
# encoding = tokenizer.encode(text)
# print("Token IDs:", encoding)

# # Convert token IDs back to tokens
# tokens = tokenizer.convert_ids_to_tokens(encoding)
# tokens=[i for i in tokens if i not in string.punctuation]
# print("Tokens:", tokens)

"""the tokens here may seen abnormal, they are not is Word piece tokenization which is pretty normal in BERT"""

# preprocessor = keras_nlp.models.BertPreprocessor.from_preset("bert_base_en_uncased",trainable=True)
# encoder = keras_nlp.models.BertBackbone.from_preset("bert_base_en_uncased")

# def get_sentence_embeding(sentences):
#     preprocessed_text = preprocessor(sentences)
#     return encoder(preprocessed_text)['pooled_output']

model = SentenceTransformer('all-MiniLM-L6-v2')

df.shape

df["embeddings"] = None # Create an empty 'embeddings' column

for i in range(df.shape[0]):
  scheme_text = df["tokens"].iloc[i] # Changed index to i
  if isinstance(scheme_text, str): # Check if scheme_text is a string
    scheme_embedding = model.encode(scheme_text, convert_to_tensor=True)
    df["embeddings"].iloc[i] = scheme_embedding # Store the embedding using iloc

df.head()



null_embeddings_indices = df[df['embeddings'].isnull()].index.tolist()
print(null_embeddings_indices)

sentence_emb = model.encode([text], convert_to_tensor=True)

for i in null_embeddings_indices:
  df['tokens'][i] = df['details'][i] + ' ' + df['benefits'][i] + ' ' + df['schemeCategory'][i]

for i in null_embeddings_indices:
  scheme_text = df["tokens"].iloc[i]
  if isinstance(scheme_text, str):
    scheme_embedding = model.encode(scheme_text, convert_to_tensor=True)
    df["embeddings"].iloc[i] = scheme_embedding

null_embeddings_indices = df[df['embeddings'].isnull()].index.tolist()

df.to_pickle("embeddings_new.pkl")

df.head()